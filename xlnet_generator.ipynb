{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xlnet-generator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "deb40fa441384146a880eef99779f33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d97af69932fb42a897ef3af58abe08ba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b6aba1c90d0546b7be9d430239437894",
              "IPY_MODEL_859d5cd8ec624570832a5eb62ac3cd4d"
            ]
          }
        },
        "d97af69932fb42a897ef3af58abe08ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6aba1c90d0546b7be9d430239437894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6fd5c3b2e7054743a273593313a0d8dc",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798011,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798011,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7cf94bbed33042ce8b66498e596e3667"
          }
        },
        "859d5cd8ec624570832a5eb62ac3cd4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_764a7815a1f0423dad37743205ef9281",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 798k/798k [01:49&lt;00:00, 7.29kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2dca01661ebe42c8a3cff1916fd369a6"
          }
        },
        "6fd5c3b2e7054743a273593313a0d8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7cf94bbed33042ce8b66498e596e3667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "764a7815a1f0423dad37743205ef9281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2dca01661ebe42c8a3cff1916fd369a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3cd0fce78b44717aacceb6eb113ed2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_099ad9fb5e49434e9ea96bcb05a6aaf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3f4f6daaa2fa4c12852c06c742feb1a4",
              "IPY_MODEL_d4d57f74bbc845c5ad820fa35d65e65c"
            ]
          }
        },
        "099ad9fb5e49434e9ea96bcb05a6aaf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f4f6daaa2fa4c12852c06c742feb1a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f388c9c20c3e47519670cc93a231b82f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 761,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 761,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aadc6a70fa1e4250bbdf51449c69a921"
          }
        },
        "d4d57f74bbc845c5ad820fa35d65e65c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0f91e429ac664c9599fea61284fb9698",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 761/761 [00:00&lt;00:00, 1.88kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b44b1ff3f3d14b51bcd0c9891354692d"
          }
        },
        "f388c9c20c3e47519670cc93a231b82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aadc6a70fa1e4250bbdf51449c69a921": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f91e429ac664c9599fea61284fb9698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b44b1ff3f3d14b51bcd0c9891354692d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd44914c5df04a43b50d204768ccb952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2673325dff304e14973a9a91ee7b4bda",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5de7683b55214bad91b0065f16e82f9d",
              "IPY_MODEL_5eecb9e7fa7b46b7b204e219ab1a36a1"
            ]
          }
        },
        "2673325dff304e14973a9a91ee7b4bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5de7683b55214bad91b0065f16e82f9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2ba350648dec4184b2c3e06257ae7d8e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1441285815,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1441285815,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2034e1facb6c4fc4803fda1d9395927a"
          }
        },
        "5eecb9e7fa7b46b7b204e219ab1a36a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59dce20fcd57473d93361451a8330988",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.44G/1.44G [01:33&lt;00:00, 15.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_813711063c0546f99b29c0162242af7d"
          }
        },
        "2ba350648dec4184b2c3e06257ae7d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2034e1facb6c4fc4803fda1d9395927a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59dce20fcd57473d93361451a8330988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "813711063c0546f99b29c0162242af7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYttFfFwRXeN"
      },
      "source": [
        "# Build a bidirectional text generator with XLNet\n",
        "\n",
        "by [Rostyslav Neskorozhenyi](https://www.linkedin.com/in/slanj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLw3Wxp0t_Ql"
      },
      "source": [
        "Current [Transformers](https://arxiv.org/abs/1706.03762) based models, like GPT-2 or even GPT-3 show incredible achievements  in the task of [text-generation](https://huggingface.co/blog/how-to-generate) (prediction of the next probable word based on the previous sequence of words). These models can create long, creative and cohesive texts, but usually they can generate text only in one direction, from left to right. I was wondering if there is a way to generate text in both directions and having some start phrase (for example \"text generation is cool\") to see what story will unfold around it. [XLNet](https://huggingface.co/transformers/model_doc/xlnet.html) was the solution: due to its using of all permutations of the input sequence factorization order this model can help to generate text in any direction.\n",
        "\n",
        "In this article we will not study in detail the internal principles of XLNet (excellent brief explanation you can find [here](https://towardsdatascience.com/xlnet-a-clever-language-modeling-solution-ab41e87798b0)). Instead, we'll start experimenting right away: we will practice a little bit in masked word prediction with XLNet, try to implement top-K bidirectional generation, and then implement a more efficient approach that combines beam search and top-K sampling.\n",
        "\n",
        "At the end of the article we will get a generator capable of creating such text based on the start phrase (which is highlighted in bold):\n",
        "\n",
        "> Following up on my initial thoughts: **text generation is cool**! It works great for creating blog header, title etc. You will need Word 2013\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_Eplcj-guRg"
      },
      "source": [
        "## Install needed modules\n",
        "\n",
        "We will conduct all our experiments in Google Collab Notebook (with GPU environment), which is available by this [link](https://colab.research.google.com/drive/1RhHiKTp0os2_q5z6pKS6vQUz0SM1EXrM), so the only module we will need to install is the excellent [Transformers](https://huggingface.co/transformers/) library.  This library provides a simple interface to XLNet, as well as to many other transformers based models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFF1bJB3RWhB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "outputId": "4566f418-e444-44e1-8317-76ec2608c855"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 2.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 14.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 25.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 24.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=d3cc602fd3928dd495c434061be3144ab14161c9a6d692222bac16496594ae56\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd96euzBg9xa"
      },
      "source": [
        "## Example of masked words prediction with XLNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KewHMWpF-4fg"
      },
      "source": [
        "One of the advantages of XLNet is that this model can perfectly cope with the prediction of several related masked words while taking into account the previous context. For example, I will mention in the text that I gave you three apples, and then ask the model to tell me who now owns some apples by feeding the model a sentence with masked words: \"\\<mask> have \\<mask> apples in hands\". As a result, we will see that the model perfectly understands who has apples and how many.\n",
        "\n",
        "Before we can start communicating with the model, we need to load it, as well as load a tokenizer that processes the incoming text into a digital form understandable for the model. In the basic form tokenization is splitting of the text into words or subwords, which then are converted to ids. Each model requires text to be tokenized in a specific way. XLNet uses SentencePiece method. You can read more about the tokenization process at the [link](https://huggingface.co/transformers/tokenizer_summary.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0LKVPYNRn2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "deb40fa441384146a880eef99779f33a",
            "d97af69932fb42a897ef3af58abe08ba",
            "b6aba1c90d0546b7be9d430239437894",
            "859d5cd8ec624570832a5eb62ac3cd4d",
            "6fd5c3b2e7054743a273593313a0d8dc",
            "7cf94bbed33042ce8b66498e596e3667",
            "764a7815a1f0423dad37743205ef9281",
            "2dca01661ebe42c8a3cff1916fd369a6",
            "c3cd0fce78b44717aacceb6eb113ed2b",
            "099ad9fb5e49434e9ea96bcb05a6aaf1",
            "3f4f6daaa2fa4c12852c06c742feb1a4",
            "d4d57f74bbc845c5ad820fa35d65e65c",
            "f388c9c20c3e47519670cc93a231b82f",
            "aadc6a70fa1e4250bbdf51449c69a921",
            "0f91e429ac664c9599fea61284fb9698",
            "b44b1ff3f3d14b51bcd0c9891354692d",
            "cd44914c5df04a43b50d204768ccb952",
            "2673325dff304e14973a9a91ee7b4bda",
            "5de7683b55214bad91b0065f16e82f9d",
            "5eecb9e7fa7b46b7b204e219ab1a36a1",
            "2ba350648dec4184b2c3e06257ae7d8e",
            "2034e1facb6c4fc4803fda1d9395927a",
            "59dce20fcd57473d93361451a8330988",
            "813711063c0546f99b29c0162242af7d"
          ]
        },
        "outputId": "827c1a81-f4fb-4338-c39b-bc68b56ff6a4"
      },
      "source": [
        "# Predict mentioned words in a sentence with XLNet\n",
        "\n",
        "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
        "import torch\n",
        "\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "deb40fa441384146a880eef99779f33a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3cd0fce78b44717aacceb6eb113ed2b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=761.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd44914c5df04a43b50d204768ccb952",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1441285815.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "potVMD7-HWv9"
      },
      "source": [
        "Also we need to add a padding text to help XLNet with short texts as was [proposed](https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e) by Aman Rusia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rToalwmxRvXF"
      },
      "source": [
        "# Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia\n",
        "# in https://github.com/rusiaaman/XLNet-gen#methodology\n",
        "# and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n",
        "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
        "(except for Alexei and Maria) are discovered.\n",
        "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
        "remainder of the story. 1883 Western Siberia,\n",
        "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
        "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
        "father initially slaps him for making such an accusation, Rasputin watches as the\n",
        "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
        "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
        "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL98ml2HI2v8"
      },
      "source": [
        "Predict top 5 words for each \\<mask> token. To make a prediction we need to feed the model with tokenized text, masked words indexes and permutation masks. Permutation masks are needed to disable input tokens to attend to masked tokens. You can read more about model parameters [here](https://huggingface.co/transformers/model_doc/xlnet.html#xlnetlmheadmodel)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHJ7Ji85VxiA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "cd17a815-8a0c-4af6-8185-f170f9fd5a7a"
      },
      "source": [
        "torch.manual_seed(0)\n",
        "# We show how to setup inputs to predict a next token using a bi-directional context.\n",
        "# We will predict masked tokens\n",
        "input_ids = torch.tensor(tokenizer.encode(PADDING_TEXT + \"I gave you three apples. <mask> have <mask> apples in hands\", add_special_tokens=False)).unsqueeze(0)  \n",
        "\n",
        "targets = [ -6, -4]\n",
        "\n",
        "perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "perm_mask[0, :, targets] = 1.0  # Previous tokens don't see last token\n",
        "\n",
        "target_mapping = torch.zeros((1, len(targets), input_ids.shape[1]), dtype=torch.float)  \n",
        "\n",
        "target_mapping[0, 0, targets[0]] = 1.0  # Our first  prediction \n",
        "target_mapping[0, 1, targets[1]] = 1.0  # Our second  prediction \n",
        "\n",
        "input_ids_tensor = input_ids.to(\"cuda\")\n",
        "target_mapping_tensor = target_mapping.to(\"cuda\")\n",
        "perm_mask_tensor = perm_mask.to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
        "next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n",
        "\n",
        "for j in range(len(targets)):\n",
        "  predicted_k_indexes = torch.topk(outputs[0][0][j],k=5)\n",
        "  predicted_logits_list = predicted_k_indexes[0] \n",
        "  predicted_indexes_list = predicted_k_indexes[1] \n",
        "    \n",
        "  print (\"predicted word:\",tokenizer.decode(input_ids[0][targets[j]].item()), j)\n",
        "  for i,item  in enumerate(predicted_indexes_list):\n",
        "      the_index = predicted_indexes_list[i].item()\n",
        "      print(\"word and logits\",tokenizer.decode(the_index),predicted_logits_list[i].item())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predicted word: <mask> 0\n",
            "word and logits You -9.070054054260254\n",
            "word and logits I -10.822368621826172\n",
            "word and logits We -12.820359230041504\n",
            "word and logits Now -14.133552551269531\n",
            "word and logits They -14.863320350646973\n",
            "predicted word: <mask> 1\n",
            "word and logits three -23.045528411865234\n",
            "word and logits the -24.3369083404541\n",
            "word and logits these -25.59902000427246\n",
            "word and logits two -25.809444427490234\n",
            "word and logits your -25.947147369384766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFuQjjGfht6F"
      },
      "source": [
        "## Top-k bi-directional generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4CeuKI3tf82"
      },
      "source": [
        "Now when we know how to predict masked words with XLNet it's time to create a top-k bidirectional text generator. Its work principles are simple. We will create a loop and at each iteration the model will predict top-k tokens for a masked word on the right or on the left side of start phrase. After that we add random token from topK to the start phrase and repeat iteration for n times.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFXPoxeYsTQj"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Function to select topK tokens from the probability list and \n",
        "# then based on the selected K word distribution get sample of random token IDs\n",
        "\n",
        "def choose_from_top(probs, k=5, sample_size=1):\n",
        "    ind = np.argpartition(probs, -k)[-k:]\n",
        "    top_prob = probs[ind]\n",
        "    # print(tokenizer.decode(ind))\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(k, sample_size, p = top_prob, replace=False)\n",
        "    token_ids = ind[choice]\n",
        "    return token_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-dLcP1iXjIx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "789d1e6d-f2b7-403d-923d-a1f7edb8a745"
      },
      "source": [
        "# top-K bidiretional generation\n",
        "\n",
        "sent = \"text generation is cool\"\n",
        "topk = 10\n",
        "n = 20\n",
        "# Lower temperatures make the model more confident in its top choices, while temperatures greater than 1 decrease confidence.\n",
        "temperature = 5\n",
        "model.eval()\n",
        "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
        "\n",
        "sent_tokens = tokenizer.encode(sent, add_special_tokens=False)\n",
        "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
        "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)\n",
        "   \n",
        "for i in range(n):\n",
        "  input = mask_tokens + sent_tokens + mask_tokens     \n",
        "  target_id1 = -len(input)\n",
        "  target_id2 = -1\n",
        "\n",
        "  input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)   # We will predict masked tokens\n",
        "\n",
        "  perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "  perm_mask[0, :, [target_id1, target_id2]] = 1.0  # Previous tokens don't see last token\n",
        "\n",
        "  target_mapping = torch.zeros((1, 2, input_ids.shape[1]), dtype=torch.float)  \n",
        "  target_mapping[0, 0, target_id1] = 1.0  # Our first  prediction \n",
        "  target_mapping[0, 1, target_id2] = 1.0  # Our second  prediction \n",
        "\n",
        "  input_ids_tensor = input_ids.to(\"cuda\")\n",
        "  target_mapping_tensor = target_mapping.to(\"cuda\")\n",
        "  perm_mask_tensor = perm_mask.to(\"cuda\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
        "\n",
        "  predicted_tokens = []\n",
        "  \n",
        "  for j in range(2):\n",
        "    probs = torch.nn.functional.softmax(outputs[0][0][j]/temperature, dim = 0).to('cpu').numpy()\n",
        "    predicted_tokens.append(choose_from_top(probs, k=topk, sample_size=1))\n",
        "\n",
        "  if i % 2 == 0:    \n",
        "    tok = predicted_tokens[0][0]\n",
        "    sent_tokens = [tok] + sent_tokens \n",
        "    print('left: ', tokenizer.decode(sent_tokens))\n",
        "  else:     \n",
        "    tok = predicted_tokens[1][0]\n",
        "    sent_tokens = sent_tokens + [tok]\n",
        "    print(\"right: \", tokenizer.decode(sent_tokens)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "left:  The text generation is cool\n",
            "right:  The text generation is cool for\n",
            "left:  ? The text generation is cool for\n",
            "right:  ? The text generation is cool for me\n",
            "left:  ? The text generation is cool for me\n",
            "right:  ? The text generation is cool for me to\n",
            "left:  :? The text generation is cool for me to\n",
            "right:  :? The text generation is cool for me to see\n",
            "left:  says:? The text generation is cool for me to see\n",
            "right:  says:? The text generation is cool for me to see and\n",
            "left:  and says:? The text generation is cool for me to see and\n",
            "right:  and says:? The text generation is cool for me to see and the\n",
            "left:  reviews and says:? The text generation is cool for me to see and the\n",
            "right:  reviews and says:? The text generation is cool for me to see and the font\n",
            "left:  User reviews and says:? The text generation is cool for me to see and the font\n",
            "right:  User reviews and says:? The text generation is cool for me to see and the font size\n",
            "left:  1 User reviews and says:? The text generation is cool for me to see and the font size\n",
            "right:  1 User reviews and says:? The text generation is cool for me to see and the font size seems\n",
            "left:  • 1 User reviews and says:? The text generation is cool for me to see and the font size seems\n",
            "right:  • 1 User reviews and says:? The text generation is cool for me to see and the font size seems right\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDgVNdAmxEbW"
      },
      "source": [
        "Not too impressive. There is a lot of repetitions and whole text looks meaningless. But we will find a better solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Cry_cj6jIbb"
      },
      "source": [
        "## Top-k-beam bi-directional text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3t2oPTTN-qj"
      },
      "source": [
        "As we can see, it is still quite difficult for the model to generate text right-to-left. We often get a word that does not fit into the context well, which leads to an even less suitable next word. As a result, the generated text becomes incoherent.\n",
        "\n",
        "We can increase the chances of finding connected word sequences by generating words not by one on each side of the starting phrase, but by creating a certain number of beams of word sequences and choosing one of the most probable beams of a certain length.\n",
        "\n",
        "Thus, we get some kind of combination of top-k sampling and beam search. The principle of the resulting method is shown in the diagram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSLgCE5GbY23"
      },
      "source": [
        "![Generation schema](https://drive.google.com/uc?id=16ZqB6g5T7dlTwcrCHaSMP5hQnAv0BCiI)\n",
        "\n",
        "Image was created by Rostyslav Neskorozhenyi with [draw.io](https://draw.io/) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaIjXBwWDTGe"
      },
      "source": [
        "The bidirectional generation process consists of n iterations. I split each iteration into four steps for better understanding:\n",
        "\n",
        "- In the first step, we get a start phrase and generate right-to-left on its left side a certain number of beams of a certain length (at each stage of beam search, we select next token candidates with top-K sampling).\n",
        "\n",
        "- In the second step, we take a random beam from the top-K most probable beams and add it to the start phrase.\n",
        "\n",
        "- The resulting new phrase serves as a start phrase for the third step, in which we generate a certain number of beams on the right side of the new start phrase.\n",
        "\n",
        "- In the fourth step, we take a random beam from the top-k beams obtained in the third step and add that beam to the new starting phrase. The resulting phrase serves as the starting point for the next iteration.\n",
        "\n",
        "I hope that the description was clear enough and the diagram will help you figure it out. The main thing is that, based on my experiments, this method in most cases allows you to generate quite coherent text bidirectionally.\n",
        "\n",
        "Let's implement the method in code. Firstly we will create a function that will take tokenized start sentence, a sequence of token candidates with their probabilities and generate next n probable sequences of token candidates on the right or on the left side. We will use this function iteratively, so generated token sequences from previous iteration will serve as input on the next iteration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WARyDqR1Or_X"
      },
      "source": [
        "# create a combination of beam and top-k generation to generate sequences of n tokens from both sides \n",
        "\n",
        "import random\n",
        "\n",
        "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)\n",
        "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
        "\n",
        "model.eval()\n",
        "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
        "\n",
        "# create a function that will take tokenized sendence and a list of token candidates (with their probabilities) \n",
        "# and generate next n probable token sequences on the right or on the left side\n",
        "\n",
        "def candidates_gen(sent_tokens, candidate=([], 1, []), d='left', n_candidates=5, topk=20, temperature=5):\n",
        "  branch_candidates = []  \n",
        "  cand_tokens = candidate[0]\n",
        "  \n",
        "  if d == 'right':    \n",
        "    input = sent_tokens + cand_tokens + mask_tokens     \n",
        "    \n",
        "    target_id = -1\n",
        "    input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)  \n",
        "\n",
        "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "    perm_mask[0, :, target_id] = 1.0  # Previous tokens don't see last token\n",
        "  else:        \n",
        "    input = mask_tokens + cand_tokens + sent_tokens    \n",
        "    \n",
        "    target_id = -len(input)  \n",
        "    input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)  \n",
        "\n",
        "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
        "    perm_mask[0, :, [target_id - i for i in range(100)]] = 1.0  # Mask additional previos tokens to improve left-side generation\n",
        "\n",
        "  # We will predict masked tokens \n",
        "  target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  \n",
        "  target_mapping[0, 0, target_id] = 1.0  # Our right  prediction \n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    input_ids_tensor = input_ids.to(\"cuda\")\n",
        "    target_mapping_tensor = target_mapping.to(\"cuda\")\n",
        "    perm_mask_tensor = perm_mask.to(\"cuda\")\n",
        "  else:\n",
        "    input_ids_tensor = input_ids\n",
        "    target_mapping_tensor = target_mapping\n",
        "    perm_mask_tensor = perm_mask\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
        "\n",
        "  probs = torch.nn.functional.softmax(outputs[0][0][0]/temperature, dim = 0)\n",
        "  selected_indexes = choose_from_top(probs.to('cpu').numpy(), k=topk, sample_size=n_candidates)\n",
        "  selected_probs = probs[selected_indexes]\n",
        "\n",
        "  # predicted_k_probs = torch.topk(probs, k = topk)\n",
        "  # predicted_indexes_list = predicted_k_probs[1]\n",
        "  # indexes = list(range(predicted_indexes_list.shape[0]))\n",
        "  # selected = random.sample(indexes, n_candidates)\n",
        "  # selected_indexes = predicted_indexes_list[selected]\n",
        "  # selected_probs = predicted_k_probs[0][selected]\n",
        "\n",
        "  for i,item  in enumerate(selected_indexes):\n",
        "      the_index = item.item()\n",
        "      if d == \"right\":\n",
        "        new_sent = cand_tokens + [the_index]\n",
        "      elif d == \"left\":\n",
        "        new_sent = [the_index] + cand_tokens\n",
        "      \n",
        "      prob = selected_probs[i].item()\n",
        "      # add word combinations to branch_candidates in format [sentence, cumulative probability, all probs]\n",
        "      branch_candidates.append((new_sent, candidate[1] * prob, candidate[2] + [prob]))\n",
        "  \n",
        "  return branch_candidates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAYgamRMzOwa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "274591e5-a024-4ab6-c85c-64b3e1f55377"
      },
      "source": [
        "# test our text branch generator\n",
        "sent = \"Text generation is cool\"\n",
        "sent_tokens = tokenizer.encode(sent, add_special_tokens=False)\n",
        "first_sample_size = 5\n",
        "beams = candidates_gen(sent_tokens=sent_tokens, d='left', n_candidates=first_sample_size, temperature=5)\n",
        "for beam in beams:\n",
        "  print(tokenizer.decode(beam[0]), beam[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" 0.00016840094758663327\n",
            "that 0.00021763828408438712\n",
            "<eop> 0.00023566206800751388\n",
            "! 0.00023651127412449569\n",
            "The 0.00018732658645603806\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbPnUukTUM-A"
      },
      "source": [
        "Now we will create **beam_gen** function that will generate a list of token beams of given length (depth) using token candidates proposed by **candidates_gen**.\n",
        "\n",
        "**beam_gen** function will return final beams list sorted by probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo_VOF9Ox3wH"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def beam_gen(sent_tokens, candidates, depth=5, d='right', sample_size=2, topk=10, temperature=5):\n",
        "  beams = candidates[:]\n",
        "  new_candidates = candidates[:]\n",
        "  while depth > 0:\n",
        "    new_candidates = []\n",
        "    for candidate in candidates:\n",
        "      for new_candidate in candidates_gen(sent_tokens, candidate, d, sample_size, topk, temperature):\n",
        "        beams.append(new_candidate)\n",
        "        new_candidates.append(new_candidate)   \n",
        "    print(\"Number of beams:\", len(new_candidates))    \n",
        "    candidates = new_candidates[:]\n",
        "    depth -= 1\n",
        "  # sort candidate beams by a sum of logaryphms of probability of each word in a beam. Which is equivalet to product of probabilities \n",
        "  sorted_beams = sorted(new_candidates, key=lambda tup: np.sum(np.log10(tup[2])), reverse=True)\n",
        "  return beams, sorted_beams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9oHn_VRU7ec"
      },
      "source": [
        "Let's gather all parts together in a **bi_gen** function. \n",
        "**bi_gen** will be able to generate text left-to-right (parameter **direction**='right'), right-to-left (parameter **direction**='left'), or in both directions (parameter **direction**='both') \n",
        "\n",
        "If **both** directions are selected, generator will work in the following way: \n",
        "generate **n_tokens** on the left side, after that - n tokens in the right side, then again n tokens on the left side and so on.\n",
        "It will repeat number of times, that is saved in **iterations** parameter.\n",
        "\n",
        "We will separately indicate in **first_sample_size** parameter the number of cadidates in the first stage of beam search. This number can be higher than the number of candidates in the next stages (specified in the variable **sample_size**), since it is important to get enough candidates for the first token, on which all subsequent sequences will be based. According to my observations, this approach increases the likelihood of generating a coherent and reasonably probable sequence of tokens.\n",
        "\n",
        "We will use high **temperature** parameter to  lower model confidence in its top token choices. This allows to make the generation more varied and not get stuck with the most likely repeating sequences of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwr_4qMkG5oR"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def bi_generator(sent, direction, first_sample_size, sample_size, n_tokens, topk, iterations, temperature):\n",
        "  sent_tokens = tokenizer.encode(sent, add_special_tokens=False) \n",
        "\n",
        "  for i in range(iterations):\n",
        "    if (i % 2 == 0 and direction == 'both') or direction == 'left':\n",
        "      print('>> left side generation')\n",
        "      candidates = candidates_gen(sent_tokens=sent_tokens, d='left', n_candidates=first_sample_size,  topk=topk, temperature=temperature)\n",
        "      beams, sorted_beams = beam_gen(sent_tokens, candidates, n_tokens-1, 'left', sample_size, topk, temperature=temperature)\n",
        "      topn = len(sorted_beams)//5 if len(sorted_beams) > 4 else len(sorted_beams)\n",
        "      selected_candidate = random.choice(sorted_beams[:topn])\n",
        "      sent_tokens = selected_candidate[0] + sent_tokens\n",
        "      print(tokenizer.decode(sent_tokens))\n",
        "    if (i % 2 != 0 and direction == 'both') or direction == 'right':\n",
        "      print('>> right side generation')\n",
        "      candidates = candidates_gen(sent_tokens=sent_tokens, d='right', n_candidates=first_sample_size, topk=topk, temperature=temperature)\n",
        "      beams, sorted_beams = beam_gen(sent_tokens, candidates, n_tokens-1, 'right', sample_size, topk, temperature=temperature)\n",
        "      topn = len(sorted_beams)//5 if len(sorted_beams) > 4 else len(sorted_beams)\n",
        "      selected_candidate = random.choice(sorted_beams[:topn])\n",
        "      sent_tokens = sent_tokens + selected_candidate[0]\n",
        "      print(tokenizer.decode(sent_tokens))\n",
        "    \n",
        "  return tokenizer.decode(sent_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3tsFaiZhTwx"
      },
      "source": [
        "And finally we will try our bidirectional text generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6pzg38fclAX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "da507f21-34f6-48d3-9db3-a6445534aa91"
      },
      "source": [
        "sent = \"James Bond\"  \n",
        "first_sample_size = 4\n",
        "sample_size = 2\n",
        "n_tokens = 4\n",
        "topk = 20\n",
        "iterations = 6\n",
        "temperature = 4\n",
        "direction = \"both\"\n",
        "\n",
        "bi_generator(sent, direction, first_sample_size, sample_size, n_tokens, topk, iterations, temperature);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> left side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "his starring role as James Bond\n",
            ">> right side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "his starring role as James Bond (2006-\n",
            ">> left side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "is last seen in his starring role as James Bond (2006-\n",
            ">> right side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "is last seen in his starring role as James Bond (2006-2014) and,\n",
            ">> left side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "Valenciennes is last seen in his starring role as James Bond (2006-2014) and,\n",
            ">> right side generation\n",
            "Number of beams: 8\n",
            "Number of beams: 16\n",
            "Number of beams: 32\n",
            "Valenciennes is last seen in his starring role as James Bond (2006-2014) and, when released from contract\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSpM79tvErSE"
      },
      "source": [
        "## Conclusion\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZjSSfCeErNJ"
      },
      "source": [
        "It's also starting to seem somewhat like we are embarking into a new world largely controlled by **artificial intelligence** based on its ability over a long period to manipulate, manage and adapt our daily lives.\n",
        "\n",
        "The entire previous paragraph was generated by our new text generator. The text is pretty convincing, isn't it? Therefore, please accept my congratulations. We have created almost the first of its kind **transformers based bidirectional text generator**. And while it still makes a lot of mistakes, it can be used to create a lot of interesting and fun stories that will grow around any phrase that comes to your mind."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAbyPvaIzuXT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poRq_Ydxptk8"
      },
      "source": [
        "**More examples of text from Bidirectional generator:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9tmjV1Aphtn"
      },
      "source": [
        "> Follow the trend: Graphic design is cool, **text generation** is cool, data manipulation and algorithms are cool, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-soVeg3wqXAy"
      },
      "source": [
        "> Theoretical scientific framework for the field is enriched across various technological disciplines and these topics include genetic programming and **machine learning**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k20d9nwCpw4F"
      },
      "source": [
        "> Most **drink some beer** and vodka everyday and have no knowledge on the importance of drinking "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PMvolFnp8n9"
      },
      "source": [
        "> Following up on my initial thoughts: **text generation is cool**! it works great for creating blog header, title etc. You will need Word 2013\n",
        "\n",
        "\n"
      ]
    }
  ]
}