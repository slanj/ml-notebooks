{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increasing-starter",
   "metadata": {},
   "source": [
    "## How to get high score using MMBT and CLIP in Hateful Memes Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-anaheim",
   "metadata": {},
   "source": [
    "#### Use CLIP as a feature encoder for Multimodal Bitransformer and make MMBT really work with Huggingface Transformers to get surprisingly high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-membership",
   "metadata": {},
   "source": [
    "The additional stage of <a href=https://www.drivendata.org/competitions/64/hateful-memes/>Hateful Memes Competition</a> from Facebook ended a few months ago. My team was lucky enough to take part in this competition and even get pretty good results (we took tenth place). How we did it and what methods we used - I'll tell you in this article. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-saturday",
   "metadata": {},
   "source": [
    "## Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-seventh",
   "metadata": {},
   "source": [
    "At first glance, the problem that had to be solved in the competition is quite simple - to determine whether a meme is hateful or not using text and image data from it. In reality, the problem is complicated by the many ambiguities inherent in our speech, as well as by the presence of sarcasm and irony, with the definition of which neural networks have problems. You can read more about the competition and the tasks it posed in the corresponding <a href=https://arxiv.org/pdf/2005.04790v2.pdf>paper</a>. \n",
    "\n",
    "<img src=https://drivendata-public-assets.s3.amazonaws.com/memes-overview.png />\n",
    "Image from <a href=https://www.drivendata.org/competitions/64/hateful-memes/>DrivenData</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-collapse",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-hazard",
   "metadata": {},
   "source": [
    "During the competition, a downloadable zip file was provided. Now competition data can be found at this <a href=https://hatefulmemeschallenge.com/>link</a>.\n",
    "\n",
    "The zip file includes a folder with images and several json files with images annotations.\n",
    "\n",
    "<b>img/</b> folder contains all the images of the challenge dataset including train, dev and test split. The images are named \\<id>.png, where \\<id> is a unique 5 digit number.  \n",
    "\n",
    "**train.jsonl, dev_seen.jsonl, dev_unseen.jsonl** — json files where each line has a dictionary of key-value pairs of data about the images. The dictionary includes\n",
    "\n",
    "- **id** The unique identifier between the img directory and the .jsonl files, e.g., \"id\": 13894.\n",
    "- **img** The actual meme filename, e.g., \"img\": img/13894.png, note that the filename contains the img directory described above, and that the filename stem is the id.\n",
    "- **text** The raw text string embedded in the meme image, e.g., img/13894.png has \"text\": \"putting bows on your pet\"\n",
    "- **label** where 1 -> \"hateful\" and 0 -> \"non-hateful\"\n",
    "\n",
    "\n",
    "For example:\n",
    "\n",
    "{\"id\":23058,\"img\":\"img\\/23058.png\",\"label\":0,\"text\":\"don't be afraid to love again everyone is not like your ex\"}\n",
    "\n",
    "**test_seen.jsnol** includes mentioned keys, except <b>label</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-zambia",
   "metadata": {},
   "source": [
    "## Performance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-anger",
   "metadata": {},
   "source": [
    "Model performance and leaderboard rankings were determined using the **AUC ROC** or the Area Under the Curve of the Receiver Operating Characteristic. The metric measures how well your binary classifier discriminates between the classes as its decision threshold is varied. \n",
    "\n",
    "<img src=images/auroc.png />\n",
    "\n",
    "Another metric was the accuracy of  predictions, given by the ratio of correct predictions to the total number of predictions made. \n",
    "\n",
    "<img src=images/accuracy.png />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-realtor",
   "metadata": {},
   "source": [
    "## Our approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-thermal",
   "metadata": {},
   "source": [
    "There are many models and frameworks for working with multimodal data, the one that stands out the most is <a href=https://mmf.sh/>MMF</a> from Facebook. <a href=https://mmf.sh/>MMF</a> provides a simple interface for accessing many powerful multimodal models. But we, as big fans of <a href=https://github.com/huggingface/transformers>Huggingface Transformers</a>, decided not to go the easy way. We decided to find out what multimodal models are available in <a href=https://github.com/huggingface/transformers>Transformers</a> and how to get the most out of them. It turned out that only one model of this kind is currently available in Transformers - <b><a href=\"https://huggingface.co/transformers/summary.html#multimodal-models\">Multimodal Bitransformer (MMBT)</a></b>. Making this model work was not so easy, but this made the task only more interesting. \n",
    "\n",
    "<img width='700px' src='images/MMBT.png'/>\n",
    "MMBT architecture, from <a href='https://arxiv.org/abs/1909.02950'>Supervised Multimodal Bitransformers for Classifying Images and Text paper</a>\n",
    "\n",
    "MMBT fuses information from text and image encoders. <a href='https://huggingface.co/transformers/model_doc/bert.html'>BERT</a> is used as text encoder and <a href='https://pytorch.org/hub/pytorch_vision_resnet/'>ResNet</a> as image encoder. We took advantage of MMBT architecture flexibility and replaced ResNet with <a href=https://github.com/openai/CLIP>CLIP</a> for image encoding.  <b>CLIP</b> pre-trains an image encoder and a text encoder to predict which images were paired with which texts in a dataset. Our assumption was that features from CLIP are more versatile and better suited for a multimodal domain.\n",
    "\n",
    "<img width='700px' src='https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png'/>\n",
    "Summary of CLIP model’s approach, from <a href='https://arxiv.org/abs/2103.00020'>Learning Transferable Visual Models From Natural Language Supervision paper</a>\n",
    "\n",
    "For text encoding we used <a href=https://huggingface.co/Hate-speech-CNERG/bert-base-uncased-hatexplain>bert-base-uncased-hatexplain</a> model which is available in Huggingface Hub. This model was created for hatespeech detection in English, so in our case features from it are better than from <a href=https://huggingface.co/bert-base-uncased>bert-base-uncased</a> that was used in MMBT initially. \n",
    "\n",
    "Final MMBT model was finetuned on a train dataset and validated on dev_seen dataset.\n",
    "\n",
    "We also augmented texts in train dataset using controlled <a href=\"https://huggingface.co/transformers/model_doc/gpt2.html\">GPT-2</a> and <a href=\"https://github.com/dsfsi/textaugment#eda-easy-data-augmentation-techniques-for-boosting-performance-on-text-classification-tasks\">Easy Data Augmentation</a> method. This increased the accuracy of our model by an additional few percent. Augmentation is beyond the scope of this article, and I will probably write about it separately if you find this article and our approach interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-sender",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-denial",
   "metadata": {},
   "source": [
    "There will be a lot of code in this section of the article, but I will try to explain all the important parts in detail for a better understanding.\n",
    "\n",
    "First, let's import the required libraries. We will need\n",
    "\n",
    "- Transformers version >=4.8.2\n",
    "- Pytorch version 1.8.1\n",
    "- torchvision 0.9.1\n",
    "- scikit-learn 0.23.2 \n",
    "- Pillow >=8.2.0\n",
    "- tqdm >= 4.60.0\n",
    "- matplotlib >= 3.3.4\n",
    "- numpy >=1.19.5\n",
    "- <a href=https://github.com/openai/CLIP>CLIP</a> (that can be installed from repository). \n",
    "\n",
    "CLIP is now <a href=https://huggingface.co/transformers/model_doc/clip.html>accesible</a> directly in Huggingface Transformers, but at the time of the implementation of our approach it was not there yet. To get the most out of our model, we also used the <a href=https://github.com/facebookresearch/madgrad>MADGRAD</a> optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coupled-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "import clip\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import copy\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    MMBTConfig,\n",
    "    MMBTModel,\n",
    "    MMBTForClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-homeless",
   "metadata": {},
   "source": [
    "Create a variable with available device, which will do all needed computations. We will need a GPU, so our device is CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neural-stadium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-fighter",
   "metadata": {},
   "source": [
    "Load CLIP model and needed preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "executed-comedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slanj/.local/lib/python3.6/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "clip_model, preprocess = clip.load(\"RN50x4\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-titanium",
   "metadata": {},
   "source": [
    "Freeze weights of CLIP feature encoder, as we will not finetune it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unavailable-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in clip_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-latest",
   "metadata": {},
   "source": [
    "Initialize needed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "common-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image_embeds = 4\n",
    "num_labels = 1\n",
    "gradient_accumulation_steps = 20\n",
    "data_dir = './dataset'\n",
    "max_seq_length = 80 \n",
    "max_grad_norm = 0.5\n",
    "train_batch_size = 16\n",
    "eval_batch_size = 16\n",
    "image_encoder_size = 288\n",
    "image_features_size = 640\n",
    "num_train_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-south",
   "metadata": {},
   "source": [
    "Create a function that will prepare an image for CLIP encoder in a special manner. This function will split image into three tiles (by height or width, depending on the aspect ratio of the image). Finally we will get four vectors after encoding (one vector for each tile and one vector for whole image that was padded to square)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "preceding-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_image(im, desired_size):\n",
    "    '''\n",
    "    Resize and slice image\n",
    "    '''\n",
    "    old_size = im.size  \n",
    "\n",
    "    ratio = float(desired_size)/min(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "    im = im.resize(new_size, Image.ANTIALIAS)\n",
    "    \n",
    "    ar = np.array(im)\n",
    "    images = []\n",
    "    if ar.shape[0] < ar.shape[1]:\n",
    "        middle = ar.shape[1] // 2\n",
    "        half = desired_size // 2\n",
    "        \n",
    "        images.append(Image.fromarray(ar[:, :desired_size]))\n",
    "        images.append(Image.fromarray(ar[:, middle-half:middle+half]))\n",
    "        images.append(Image.fromarray(ar[:, ar.shape[1]-desired_size:ar.shape[1]]))\n",
    "    else:\n",
    "        middle = ar.shape[0] // 2\n",
    "        half = desired_size // 2\n",
    "        \n",
    "        images.append(Image.fromarray(ar[:desired_size, :]))\n",
    "        images.append(Image.fromarray(ar[middle-half:middle+half, :]))\n",
    "        images.append(Image.fromarray(ar[ar.shape[0]-desired_size:ar.shape[0], :]))\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "directed-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_pad_image(im, desired_size):\n",
    "    '''\n",
    "    Resize and pad image to a desired size\n",
    "    '''\n",
    "    old_size = im.size  \n",
    "\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "    im = im.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    # create a new image and paste the resized on it\n",
    "    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n",
    "    new_im.paste(im, ((desired_size-new_size[0])//2,\n",
    "                        (desired_size-new_size[1])//2))\n",
    "\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-rings",
   "metadata": {},
   "source": [
    "Define a function, that will get image features from CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "practical-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipEncoderMulti(nn.Module):\n",
    "    def __init__(self, num_embeds, num_features=image_features_size):\n",
    "        super().__init__()        \n",
    "        self.model = clip_model\n",
    "        self.num_embeds = num_embeds\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 4x3x288x288 -> 1x4x640\n",
    "        out = self.model.encode_image(x.view(-1,3,288,288))\n",
    "        out = out.view(-1, self.num_embeds, self.num_features).float()\n",
    "        return out  # Bx4x640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "according-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_encoder = ClipEncoderMulti(4)\n",
    "# image = Image.open(\"dataset/img/42953.png\").convert(\"RGB\")\n",
    "\n",
    "# sliced_images = slice_image(image, 288)\n",
    "# sliced_images = [np.array(preprocess(im)) for im in sliced_images] \n",
    "\n",
    "# image = resize_pad_image(image, image_encoder_size)\n",
    "# image = np.array(preprocess(image))\n",
    "\n",
    "# sliced_images = [image] + sliced_images\n",
    "# sliced_images = torch.from_numpy(np.array(sliced_images)).to(device)\n",
    "\n",
    "# print(sliced_images.shape)\n",
    "# print(image_encoder(sliced_images).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "center-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = Image.open(\"dataset/img/01576.png\").convert(\"RGB\")\n",
    "# sliced_images = slice_image(im, 288) \n",
    "# for img in sliced_images:\n",
    "#     plt.figure()\n",
    "#     plt.imshow(img)\n",
    "#     print(img.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-electric",
   "metadata": {},
   "source": [
    "Create JsonlDataset class that will load texts and preprocessed images. <b>collate_fn</b> will group data from dataset in a format needed for our pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blind-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonlDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, transforms, max_seq_length):\n",
    "        self.data = [json.loads(l) for l in open(data_path)]\n",
    "        self.data_dir = os.path.dirname(data_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = torch.LongTensor(self.tokenizer.encode(self.data[index][\"text\"], add_special_tokens=True))\n",
    "        start_token, sentence, end_token = sentence[0], sentence[1:-1], sentence[-1]\n",
    "        sentence = sentence[:self.max_seq_length]\n",
    "\n",
    "        label = torch.FloatTensor([self.data[index][\"label\"]])\n",
    "\n",
    "        image = Image.open(os.path.join(self.data_dir, self.data[index][\"img\"])).convert(\"RGB\")\n",
    "        sliced_images = slice_image(image, 288)\n",
    "        sliced_images = [np.array(self.transforms(im)) for im in sliced_images]\n",
    "        image = resize_pad_image(image, image_encoder_size)\n",
    "        image = np.array(self.transforms(image))\n",
    "        \n",
    "        sliced_images = [image] + sliced_images         \n",
    "        sliced_images = torch.from_numpy(np.array(sliced_images)).to(device)\n",
    "\n",
    "        return {\n",
    "            \"image_start_token\": start_token,            \n",
    "            \"image_end_token\": end_token,\n",
    "            \"sentence\": sentence,\n",
    "            \"image\": sliced_images,\n",
    "            \"label\": label            \n",
    "        }\n",
    "\n",
    "    def get_label_frequencies(self):\n",
    "        label_freqs = Counter()\n",
    "        for row in self.data:\n",
    "            label_freqs.update([row[\"label\"]])\n",
    "        return label_freqs\n",
    "    \n",
    "    def get_labels(self):\n",
    "        labels = []\n",
    "        for row in self.data:\n",
    "            labels.append(row[\"label\"])\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "restricted-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    lens = [len(row[\"sentence\"]) for row in batch]\n",
    "    bsz, max_seq_len = len(batch), max(lens)\n",
    "\n",
    "    mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
    "    text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
    "\n",
    "    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
    "        text_tensor[i_batch, :length] = input_row[\"sentence\"]\n",
    "        mask_tensor[i_batch, :length] = 1\n",
    "    \n",
    "    img_tensor = torch.stack([row[\"image\"] for row in batch])\n",
    "    tgt_tensor = torch.stack([row[\"label\"] for row in batch])\n",
    "    img_start_token = torch.stack([row[\"image_start_token\"] for row in batch])\n",
    "    img_end_token = torch.stack([row[\"image_end_token\"] for row in batch])\n",
    "\n",
    "    return text_tensor, mask_tensor, img_tensor, img_start_token, img_end_token, tgt_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-plaza",
   "metadata": {},
   "source": [
    "Define **load_examples** function that will load data described in json dataset into JsonlDataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "valuable-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(tokenizer, evaluate=False):\n",
    "    path = os.path.join(data_dir, \"dev_seen_clean.jsonl\" if evaluate else f\"train_augmented.jsonl\")\n",
    "    transforms = preprocess\n",
    "    dataset = JsonlDataset(path, tokenizer, transforms, max_seq_length - num_image_embeds - 2)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-failure",
   "metadata": {},
   "source": [
    "Create functions to load and save model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "offshore-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "\n",
    "    if save_path == None:\n",
    "        return\n",
    "    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "    \n",
    "def load_checkpoint(load_path, model):\n",
    "    \n",
    "    if load_path==None:\n",
    "        return\n",
    "    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-liverpool",
   "metadata": {},
   "source": [
    "Needed functions and classes are created, so we can load our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "conservative-priest",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Hate-speech-CNERG/bert-base-uncased-hatexplain were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Hate-speech-CNERG/bert-base-uncased-hatexplain'\n",
    "transformer_config = AutoConfig.from_pretrained(model_name) \n",
    "transformer = AutoModel.from_pretrained(model_name, config=transformer_config)\n",
    "img_encoder = ClipEncoderMulti(num_image_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "periodic-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "compact-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MMBTConfig(transformer_config, num_labels=num_labels, modal_hidden_size=image_features_size)\n",
    "model = MMBTForClassification(config, transformer, img_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secure-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-mattress",
   "metadata": {},
   "source": [
    "Load train and evaluation datasets and create dataloaders for these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "future-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_examples(tokenizer, evaluate=False)\n",
    "eval_dataset = load_examples(tokenizer, evaluate=True)   \n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "eval_sampler = SequentialSampler(eval_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=train_batch_size,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "        eval_dataset, \n",
    "        sampler=eval_sampler, \n",
    "        batch_size=eval_batch_size, \n",
    "        collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-nashville",
   "metadata": {},
   "source": [
    "Define model training parameters, optimizer and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "collective-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \n",
    "            \"LayerNorm.weight\"\n",
    "           ]\n",
    "weight_decay = 0.0005\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "t_total = (len(train_dataloader) // gradient_accumulation_steps) * num_train_epochs\n",
    "warmup_steps = t_total // 10\n",
    "\n",
    "optimizer = MADGRAD(optimizer_grouped_parameters, lr=2e-4)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, warmup_steps, t_total\n",
    "    )\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-malawi",
   "metadata": {},
   "source": [
    "Define evaluation function that will take evaluation dataloader and calculate prediction AUC, <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\">F1 score</a> and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pending-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, criterion, dataloader, tres = 0.5): \n",
    "    \n",
    "    # Eval!\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    proba = None\n",
    "    out_label_ids = None\n",
    "    for batch in dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            labels = batch[5]\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"input_modal\": batch[2],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"modal_start_tokens\": batch[3],\n",
    "                \"modal_end_tokens\": batch[4],\n",
    "                \"return_dict\": False\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "            tmp_eval_loss = criterion(logits, labels)\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = torch.sigmoid(logits).detach().cpu().numpy() > tres\n",
    "            proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            out_label_ids = labels.detach().cpu().numpy()\n",
    "        else:            \n",
    "            preds = np.append(preds, torch.sigmoid(logits).detach().cpu().numpy() > tres, axis=0)\n",
    "            proba = np.append(proba, torch.sigmoid(logits).detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, labels.detach().cpu().numpy(), axis=0)\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    result = {\n",
    "        \"loss\": eval_loss,\n",
    "        \"accuracy\": accuracy_score(out_label_ids, preds),\n",
    "        \"AUC\": roc_auc_score(out_label_ids, proba),\n",
    "        \"micro_f1\": f1_score(out_label_ids, preds, average=\"micro\"),\n",
    "        \"prediction\": preds,\n",
    "        \"labels\": out_label_ids,\n",
    "        \"proba\": proba\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-creator",
   "metadata": {},
   "source": [
    "Finally we can train our model. We specify minimun needed AUC value in <b>best_valid_auc</b> variable, so if the model achieves higher AUC on validation data than it was specified, we will save that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "continuous-asbestos",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 from 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c114e0bb09b04171b3ea1af26c7fcaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0333 Val loss: 0.6917 Val acc: 0.5520 AUC: 0.5830\n",
      "Train loss: 0.0294 Val loss: 0.6883 Val acc: 0.6220 AUC: 0.6403\n",
      "Train loss: 0.0268 Val loss: 0.6632 Val acc: 0.6280 AUC: 0.6857\n",
      "Train loss: 0.0273 Val loss: 0.6765 Val acc: 0.6280 AUC: 0.6928\n",
      "Train loss: 0.0243 Val loss: 0.7969 Val acc: 0.6140 AUC: 0.7083\n",
      "Train loss: 0.0235 Val loss: 0.6547 Val acc: 0.6480 AUC: 0.7308\n",
      "AUC improved, so saving this model\n",
      "Model saved to ==> models//model-embs4-seq80-auc0.752-loss0.697-acc0.640.pt\n",
      "Train loss: 0.0207 Val loss: 0.6973 Val acc: 0.6400 AUC: 0.7519\n",
      "\n",
      "\n",
      "Epoch 2 from 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78afde24f4f74303b5b1f67f7601b4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC improved, so saving this model\n",
      "Model saved to ==> models//model-embs4-seq80-auc0.770-loss0.629-acc0.702.pt\n",
      "Train loss: 0.0175 Val loss: 0.6294 Val acc: 0.7020 AUC: 0.7704\n",
      "Train loss: 0.0169 Val loss: 0.7122 Val acc: 0.6640 AUC: 0.7406\n",
      "Train loss: 0.0171 Val loss: 0.6680 Val acc: 0.7020 AUC: 0.7646\n",
      "Train loss: 0.0142 Val loss: 0.7179 Val acc: 0.6880 AUC: 0.7683\n",
      "Train loss: 0.0137 Val loss: 0.8370 Val acc: 0.6860 AUC: 0.7683\n",
      "AUC improved, so saving this model\n",
      "Model saved to ==> models//model-embs4-seq80-auc0.780-loss0.756-acc0.688.pt\n",
      "Train loss: 0.0118 Val loss: 0.7561 Val acc: 0.6880 AUC: 0.7804\n",
      "Train loss: 0.0115 Val loss: 0.8568 Val acc: 0.6760 AUC: 0.7724\n",
      "\n",
      "\n",
      "Epoch 3 from 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5b14b2a2a9c46b39d9a231ca752c515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0073 Val loss: 0.9088 Val acc: 0.6740 AUC: 0.7525\n",
      "Train loss: 0.0085 Val loss: 0.8848 Val acc: 0.6840 AUC: 0.7539\n",
      "Train loss: 0.0090 Val loss: 0.9503 Val acc: 0.7080 AUC: 0.7649\n",
      "Train loss: 0.0074 Val loss: 0.9877 Val acc: 0.6800 AUC: 0.7647\n",
      "Train loss: 0.0070 Val loss: 0.9187 Val acc: 0.7040 AUC: 0.7677\n",
      "Train loss: 0.0088 Val loss: 0.8072 Val acc: 0.6920 AUC: 0.7673\n",
      "Train loss: 0.0069 Val loss: 1.1613 Val acc: 0.6700 AUC: 0.7707\n",
      "\n",
      "\n",
      "Epoch 4 from 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ade4d729eb4aac914509d6dc79889e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0055 Val loss: 1.2092 Val acc: 0.6740 AUC: 0.7649\n",
      "Train loss: 0.0044 Val loss: 1.1423 Val acc: 0.6980 AUC: 0.7642\n",
      "Train loss: 0.0043 Val loss: 1.1668 Val acc: 0.6820 AUC: 0.7556\n",
      "Train loss: 0.0051 Val loss: 1.3731 Val acc: 0.6560 AUC: 0.7570\n",
      "Train loss: 0.0036 Val loss: 1.2333 Val acc: 0.6780 AUC: 0.7612\n",
      "Train loss: 0.0037 Val loss: 1.3292 Val acc: 0.6800 AUC: 0.7599\n",
      "Train loss: 0.0036 Val loss: 1.2982 Val acc: 0.6840 AUC: 0.7596\n",
      "\n",
      "\n",
      "Epoch 5 from 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9333026d49e449da93143b066d707c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0022 Val loss: 1.3513 Val acc: 0.6800 AUC: 0.7526\n",
      "Train loss: 0.0024 Val loss: 1.4143 Val acc: 0.6840 AUC: 0.7638\n",
      "Train loss: 0.0020 Val loss: 1.4267 Val acc: 0.6900 AUC: 0.7704\n",
      "Train loss: 0.0020 Val loss: 1.3763 Val acc: 0.6920 AUC: 0.7646\n",
      "Train loss: 0.0018 Val loss: 1.5179 Val acc: 0.6900 AUC: 0.7626\n",
      "Train loss: 0.0019 Val loss: 1.4299 Val acc: 0.6800 AUC: 0.7563\n",
      "Train loss: 0.0015 Val loss: 1.4785 Val acc: 0.6880 AUC: 0.7631\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer_step = 0\n",
    "global_step = 0\n",
    "train_step = 0\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "best_valid_auc = 0.75\n",
    "global_steps_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "val_acc_list = []\n",
    "val_auc_list = []\n",
    "eval_every = len(train_dataloader) // 7\n",
    "running_loss = 0\n",
    "file_path=\"models/\"\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for i in range(num_train_epochs):\n",
    "    print(\"Epoch\", i+1, f\"from {num_train_epochs}\")\n",
    "    whole_y_pred=np.array([])\n",
    "    whole_y_t=np.array([])\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        model.train()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        labels = batch[5]\n",
    "        inputs = {\n",
    "            \"input_ids\": batch[0],\n",
    "            \"input_modal\": batch[2],\n",
    "            \"attention_mask\": batch[1],\n",
    "            \"modal_start_tokens\": batch[3],\n",
    "            \"modal_end_tokens\": batch[4],\n",
    "            \"return_dict\": False\n",
    "        }\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "        loss = criterion(logits, labels)        \n",
    "        \n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            \n",
    "        loss.backward()\n",
    "        \n",
    "        tr_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        global_step += 1\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule         \n",
    "            \n",
    "            optimizer_step += 1\n",
    "            optimizer.zero_grad()   \n",
    "                        \n",
    "        if (step + 1) % eval_every == 0:\n",
    "            \n",
    "            average_train_loss = running_loss / eval_every\n",
    "            train_loss_list.append(average_train_loss)\n",
    "            global_steps_list.append(global_step)\n",
    "            running_loss = 0.0  \n",
    "            \n",
    "            val_result = evaluate(model, tokenizer, criterion, eval_dataloader)\n",
    "            \n",
    "            val_loss_list.append(val_result['loss'])\n",
    "            val_acc_list.append(val_result['accuracy'])\n",
    "            val_auc_list.append(val_result['AUC'])\n",
    "            \n",
    "            # checkpoint\n",
    "            if val_result['AUC'] > best_valid_auc:\n",
    "                best_valid_auc = val_result['AUC']\n",
    "                val_loss = val_result['loss']\n",
    "                val_acc = val_result['accuracy']\n",
    "                model_path = f'{file_path}/model-embs{num_image_embeds}-seq{max_seq_length}-auc{best_valid_auc:.3f}-loss{val_loss:.3f}-acc{val_acc:.3f}.pt'\n",
    "                print(f\"AUC improved, so saving this model\")  \n",
    "                save_checkpoint(model_path, model, val_result['loss'])              \n",
    "            \n",
    "            print(\"Train loss:\", f\"{average_train_loss:.4f}\", \n",
    "                  \"Val loss:\", f\"{val_result['loss']:.4f}\",\n",
    "                  \"Val acc:\", f\"{val_result['accuracy']:.4f}\",\n",
    "                  \"AUC:\", f\"{val_result['AUC']:.4f}\")   \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-norman",
   "metadata": {},
   "source": [
    "After training is complete, we can visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "british-introduction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(global_steps_list, val_auc_list)\n",
    "plt.grid()\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('MMBT Area Under the Curve')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-ending",
   "metadata": {},
   "source": [
    "## Make predictions for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-physiology",
   "metadata": {},
   "source": [
    "Now we can make a prediction for test data. Firstly we will create needed classes and functions for data loading and processing by analogy with the training stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "helpful-imaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "functional-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 1\n",
    "data_dir = './dataset'\n",
    "test_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ethical-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestJsonlDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, transforms, max_seq_length):\n",
    "        self.data = [json.loads(l) for l in open(data_path)]\n",
    "        self.data_dir = os.path.dirname(data_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = torch.LongTensor(self.tokenizer.encode(self.data[index][\"text\"], add_special_tokens=True))\n",
    "        start_token, sentence, end_token = sentence[0], sentence[1:-1], sentence[-1]\n",
    "        sentence = sentence[:self.max_seq_length]\n",
    "\n",
    "        id = torch.LongTensor([self.data[index][\"id\"]])        \n",
    "        image = Image.open(os.path.join(self.data_dir, self.data[index][\"img\"])).convert(\"RGB\")\n",
    "        sliced_images = slice_image(image, 288)\n",
    "        sliced_images = [np.array(self.transforms(im)) for im in sliced_images]\n",
    "        image = resize_pad_image(image, image_encoder_size)\n",
    "        image = np.array(self.transforms(image))        \n",
    "        sliced_images = [image] + sliced_images        \n",
    "        sliced_images = torch.from_numpy(np.array(sliced_images)).to(device)\n",
    "\n",
    "        return {\n",
    "            \"image_start_token\": start_token,            \n",
    "            \"image_end_token\": end_token,\n",
    "            \"sentence\": sentence,\n",
    "            \"image\": sliced_images,\n",
    "            \"id\": id,\n",
    "        }\n",
    "    \n",
    "def final_collate_fn(batch):\n",
    "    lens = [len(row[\"sentence\"]) for row in batch]\n",
    "    bsz, max_seq_len = len(batch), max(lens)\n",
    "\n",
    "    mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
    "    text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
    "\n",
    "    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
    "        text_tensor[i_batch, :length] = input_row[\"sentence\"]\n",
    "        mask_tensor[i_batch, :length] = 1\n",
    "\n",
    "    img_tensor = torch.stack([row[\"image\"] for row in batch])\n",
    "    id_tensor = torch.stack([row[\"id\"] for row in batch])\n",
    "    img_start_token = torch.stack([row[\"image_start_token\"] for row in batch])\n",
    "    img_end_token = torch.stack([row[\"image_end_token\"] for row in batch])\n",
    "\n",
    "    return text_tensor, mask_tensor, img_tensor, img_start_token, img_end_token, id_tensor\n",
    "\n",
    "def load_test_examples(test_file=\"test_seen.jsonl\"):\n",
    "    path = os.path.join(data_dir, test_file)\n",
    "    dataset = TestJsonlDataset(path, tokenizer, preprocess, max_seq_length - num_image_embeds - 2)\n",
    "    return dataset\n",
    "\n",
    "def final_prediction(model, dataloader): \n",
    "    preds = None\n",
    "    proba = None\n",
    "    all_ids = None\n",
    "    for batch in tqdm(dataloader):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            ids = batch[5]\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"input_modal\": batch[2],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"modal_start_tokens\": batch[3],\n",
    "                \"modal_end_tokens\": batch[4],\n",
    "                \"return_dict\": False\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs[0]\n",
    "        if preds is None:\n",
    "            all_ids = ids.detach().cpu().numpy()\n",
    "            preds = torch.sigmoid(logits).detach().cpu().numpy() > 0.5\n",
    "            proba = torch.sigmoid(logits).detach().cpu().numpy()            \n",
    "        else:  \n",
    "            all_ids = np.append(all_ids, ids.detach().cpu().numpy(), axis=0)\n",
    "            preds = np.append(preds, torch.sigmoid(logits).detach().cpu().numpy() > 0.5, axis=0)\n",
    "            proba = np.append(proba, torch.sigmoid(logits).detach().cpu().numpy(), axis=0)\n",
    "    \n",
    "    result = {\n",
    "        \"ids\": all_ids,\n",
    "        \"preds\": preds,\n",
    "        \"probs\": proba,\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cellular-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = load_test_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "coastal-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_sampler = SequentialSampler(final_test)\n",
    "\n",
    "final_test_dataloader = DataLoader(\n",
    "        final_test, \n",
    "        sampler=final_test_sampler, \n",
    "        batch_size=test_batch_size, \n",
    "        collate_fn=final_collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-antique",
   "metadata": {},
   "source": [
    "Finally we make a prediction and save it in a specified format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "palestinian-seeking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc84d790a1c147369eddb93a2a99cbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = final_prediction(model, final_test_dataloader)\n",
    "\n",
    "results['ids'] = results['ids'].reshape(-1)\n",
    "results['preds'] = results['preds'].reshape(-1)\n",
    "results['probs'] = results['probs'].reshape(-1)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df[['ids', 'probs', 'preds']]\n",
    "df.columns = ['id', 'proba', 'label']\n",
    "df.label = df.label.astype(int)\n",
    "\n",
    "df.to_csv('final_multimodal_prediction.csv', index=False, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-meaning",
   "metadata": {},
   "source": [
    "Thats it. In case the competition reopens, this prediction file can be sent there immediately. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-afternoon",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-flavor",
   "metadata": {},
   "source": [
    "In this article I tried to describe in detail the concept and implementation of the approach that we used in Hateful Memes Competition from Facebook. The tasks that the competition set for us turned out to be extremely interesting and we had a lot of fun developing our approach to solve these tasks. I hope you enjoyed reading this article too. \n",
    "\n",
    "I also want to mention that to obtain maximum AUC, we combined the prediction of several variants of this model trained with different loss parameters and different augmentation options. But that's a topic for an entirely different article. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
